@book{esl,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2ed},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2009},
  publisher={Springer Science \& Business Media}
}

@article{bartlett2020benign,
  title={Benign Overfitting in Linear Regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={arXiv:1906.11300},
  year={2019}
}

@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv:1903.08560},
  year={2019}
}

@article{rauhut2016interpolation,
  title={Interpolation via weighted $\ell$1 minimization},
  author={Rauhut, Holger and Ward, Rachel},
  journal={Applied and Computational Harmonic Analysis},
  volume={40},
  number={2},
  pages={321--351},
  year={2016},
  publisher={Elsevier}
}

@article{bah2016sample,
  title={The sample complexity of weighted sparse approximation},
  author={Bah, Bubacarr and Ward, Rachel},
  journal={IEEE Transactions on Signal Processing},
  volume={64},
  number={12},
  pages={3145--3155},
  year={2016},
  publisher={IEEE}
}

@article{products,
      title={Data Analysis Recipes: Products of multivariate Gaussians in Bayesian inferences}, 
      author={David W. Hogg and Adrian M. Price-Whelan and Boris Leistedt},
      year={2020},
      journal={arXiv:2005.14199},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

@article{xie2020weighted,
  title={Weighted optimization: better generalization by smoother interpolation},
  author={Xie, Yuege and Ward, Rachel and Rauhut, Holger and Chou, Hung-Hsu},
  journal={arXiv:2006.08495},
  year={2020}
}

@article{rahimi2007random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={20},
  pages={1177--1184},
  year={2007}
}


@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv:1908.05355},
  year={2019}
}

@article{solve,
      title={How Accurate is inv(A)*b?}, 
      author={Alex Druinsky and Sivan Toledo},
      year={2012},
      journal={arXiv:1201.6035},
      archivePrefix={arXiv},
      primaryClass={cs.NA}
}

@article{jain198239,
  title={Dimensionality and sample size considerations in pattern recognition practice},
  author={Jain, Anil K and Chandrasekaran, Balakrishnan},
  journal={Handbook of Statistics},
  volume={2},
  pages={835--855},
  year={1982},
  publisher={Elsevier}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{spigler2018jamming,
  title={A jamming transition from under-to over-parametrization affects loss landscape and generalization},
  author={Spigler, Stefano and Geiger, Mario and d'Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
  journal={arXiv:1810.09665},
  year={2018}
}


@article{geiger2019jamming,
  title={Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and d'Ascoli, St{\'e}phane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
  journal={Physical Review E},
  volume={100},
  number={1},
  pages={012115},
  year={2019},
  publisher={APS}
}

@book{gelman,
  title={Regression and Other Stories},
  author={Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year={2020},
  publisher={Cambridge University Press}
}

@book{agresti,
  title={Foundations of Linear and Generalized Linear Models},
  author={Agresti, Alan},
  year={2015},
  publisher={John Wiley \& Sons}
}

@book{bishop,
  title={Pattern Recognition and Machine Learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={Springer}
}

@article{fitting,
      title={Data analysis recipes: Fitting a model to data}, 
      author={David W. Hogg and Jo Bovy and Dustin Lang},
      year={2010},
      journal={arXiv:1008.4686},
      archivePrefix={arXiv},
      primaryClass={astro-ph.IM}
}

@article{cv,
author = {Stone, M.},
title = {Cross-Validatory Choice and Assessment of Statistical Predictions},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {36},
number = {2},
pages = {111-133},
keywords = {crossvalidation, prescription, doublecross, choice of variables, modelmix, prediction, univariate estimation, multiple regression, analysis of variance},
doi = {https://doi.org/10.1111/j.2517-6161.1974.tb00994.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1974.tb00994.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1974.tb00994.x},
abstract = {Summary A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
year = {1974}
}

@article{krige,
  title={A statistical approach to some basic mine valuation problems on the Witwatersrand},
  author={Krige, Daniel G},
  journal={Journal of the Southern African Institute of Mining and Metallurgy},
  volume={52},
  number={6},
  pages={119--139},
  year={1951},
  publisher={Southern African Institute of Mining and Metallurgy}
}

@ARTICLE{zaroubi,
       author = {{Zaroubi}, S. and {Hoffman}, Y. and {Fisher}, K.~B. and {Lahav}, O.},
        title = "{Wiener Reconstruction of the Large-Scale Structure}",
      journal = {The Astrophysical Journal},
     keywords = {COSMOLOGY: LARGE-SCALE STRUCTURE OF UNIVERSE, COSMOLOGY: THEORY, Astrophysics},
         year = 1995,
       volume = {449},
        pages = {446},
          doi = {10.1086/176070},
archivePrefix = {arXiv},
       eprint = {astro-ph/9410080},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1995ApJ...449..446Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{celerite,
       author = {{Foreman-Mackey}, Daniel and {Agol}, Eric and {Ambikasaran}, Sivaram and {Angus}, Ruth},
        title = "{Fast and Scalable Gaussian Process Modeling with Applications to Astronomical Time Series}",
      journal = {The Astronomical Journal},
     keywords = {asteroseismology, methods: data analysis, methods: statistical, planetary systems, stars: rotation, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Solar and Stellar Astrophysics, Physics - Data Analysis, Statistics and Probability, Statistics - Applications},
         year = 2017,
       volume = {154},
       number = {6},
          eid = {220},
        pages = {220},
          doi = {10.3847/1538-3881/aa9332},
archivePrefix = {arXiv},
       eprint = {1703.09710},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017AJ....154..220F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{aigrain,
       author = {{Aigrain}, S. and {Parviainen}, H. and {Pope}, B.~J.~S.},
        title = "{K2SC: flexible systematics correction and detrending of K2 light curves using Gaussian process regression}",
      journal = {Monthly Notices of the Royal Astronomical Society},
     keywords = {methods: data analysis, planetary systems, stars: rotation, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = 2016,
       volume = {459},
       number = {3},
        pages = {2408-2419},
          doi = {10.1093/mnras/stw706},
archivePrefix = {arXiv},
       eprint = {1603.09167},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.459.2408A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{huang2020dimensionality,
  title={Dimensionality reduction, regularization, and generalization in overparameterized regressions},
  author={Huang, Ningyuan and Hogg, David W and Villar, Soledad},
  journal={arXiv:2011.11477},
  year={2020}
}

@article{bootjack,
author = "Efron, B.",
doi = "10.1214/aos/1176344552",
journal = "Annals of Statistics",
sjournal = "Ann. Statist.",
number = "1",
pages = "1--26",
publisher = "The Institute of Mathematical Statistics",
title = "Bootstrap Methods: Another Look at the Jackknife",
url = "https://doi.org/10.1214/aos/1176344552",
volume = "7",
year = "1979"
}

@article{deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{dobriban2018high,
  title={High-dimensional asymptotics of prediction: Ridge regression and classification},
  author={Dobriban, Edgar and Wager, Stefan and others},
  journal={The Annals of Statistics},
  volume={46},
  number={1},
  pages={247--279},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}
@article{li2020generalization,
  title={Generalization error of minimum weighted norm and kernel interpolation},
  author={Li, Weilin},
  journal={arXiv preprint arXiv:2008.03365},
  year={2020}
}

@article{lu2002inverses,
  title={Inverses of 2$\times$ 2 block matrices},
  author={Lu, Tzon-Tzer and Shiou, Sheng-Hua},
  journal={Computers \& Mathematics with Applications},
  volume={43},
  number={1-2},
  pages={119--129},
  year={2002},
  publisher={Elsevier}
}
@article{henderson1981deriving,
  title={On deriving the inverse of a sum of matrices},
  author={Henderson, Harold V and Searle, Shayle R},
  journal={Siam Review},
  volume={23},
  number={1},
  pages={53--60},
  year={1981},
  publisher={SIAM}
}
@inproceedings{cv-bias,
  title={Ridge Regression: Structure, Cross-Validation, and Sketching},
  author={Liu, Sifan and Dobriban, Edgar},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@book{folland2016course,
  title={A course in abstract harmonic analysis},
  author={Folland, Gerald B},
  year={1994},
  publisher={CRC press}
}

@inproceedings{minh2006mercer,
  title={Mercer’s theorem, feature maps, and smoothing},
  author={Minh, Ha Quang and Niyogi, Partha and Yao, Yuan},
  booktitle={International Conference on Computational Learning Theory},
  pages={154--168},
  year={2006},
  organization={Springer}
}

@article{earlystop,
  title={On early stopping in gradient descent learning},
  author={Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
  journal={Constructive Approximation},
  volume={26},
  number={2},
  pages={289--315},
  year={2007},
  publisher={Springer}
}

@article{dropout,
  title={Dropout: A simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{lasso,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}

@ARTICLE{thecannon,
       author = {{Ness}, M. and {Hogg}, David W. and {Rix}, H. -W. and {Ho}, Anna. Y.~Q. and {Zasowski}, G.},
        title = "{The Cannon: A data-driven approach to Stellar Label Determination}",
      journal = {The Astrophysical Journal},
     keywords = {methods: data analysis, methods: statistical, stars: abundances, stars: fundamental parameters, surveys, techniques: spectroscopic, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = 2015,
       volume = {808},
       number = {1},
          eid = {16},
        pages = {16},
          doi = {10.1088/0004-637X/808/1/16},
archivePrefix = {arXiv},
       eprint = {1501.07604},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...808...16N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{cifar10,
  title={Do CIFAR-10 classifiers generalize to CIFAR-10?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  journal={arXiv:1806.00451},
  year={2018}
}

@book{gpml,
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
title = {Gaussian Processes for Machine Learning},
year = {2005},
isbn = {026218253X},
publisher = {The MIT Press}
}

@article{ft,
author = {Epstein, Charles L.},
title = {How well does the finite Fourier transform approximate the Fourier transform?},
journal = {Communications on Pure and Applied Mathematics},
volume = {58},
number = {10},
pages = {1421-1435},
doi = {https://doi.org/10.1002/cpa.20064},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20064},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.20064},
abstract = {Abstract We show that the answer to the question in the title is “very well indeed.” In particular, we prove that, throughout the maximum possible range, the finite Fourier coefficients provide a good approximation to the Fourier coefficients of a piecewise continuous function. For a continuous periodic function, the size of the error is estimated in terms of the modulus of continuity of the function. The estimates improve commensurately as the functions become smoother. We also show that the partial sums of the finite Fourier transform provide essentially as good an approximation to the function and its derivatives as the partial sums of the ordinary Fourier series. Along the way we establish analogues of the Riemann-Lebesgue lemma and the localization principle. © 2004 Wiley Periodicals, Inc.},
year = {2005}
}
